"""
Script de Detecci√≥n de Defectos en Soldadura (RSW)
Arquitectura: Procesos Gaussianos de Clasificaci√≥n (GPC) con ARD
Basado en: Informe de Investigaci√≥n Estrat√©gica para Reg√≠menes de Datos Escasos (Small Data).

Cambios principales respecto a versi√≥n anterior:
- Eliminaci√≥n de SMOTE (prevenci√≥n de distorsi√≥n de manifold).
- Eliminaci√≥n de RandomForest/XGBoost.
- Inclusi√≥n de StandardScaler dentro del bucle de validaci√≥n.
- Uso de Kernel Matern Anisotr√≥pico (ARD) + WhiteKernel.
- Optimizaci√≥n orientada a Probabilidades Calibradas.
"""

# ==============================================================================
# 1. IMPORTACIONES
# ==============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import tkinter as tk
from tkinter import filedialog
import time

# --- Funciones cient√≠ficas y estad√≠sticas ---
from scipy.signal import find_peaks
from scipy.stats import skew, kurtosis

# --- Scikit-learn Core ---
from sklearn.model_selection import (
    train_test_split, GridSearchCV, cross_val_predict, RepeatedStratifiedKFold
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    fbeta_score, make_scorer, classification_report, confusion_matrix,
    precision_score, recall_score, roc_curve, roc_auc_score
)

# --- GPC y Kernels ---
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel
from sklearn.model_selection import StratifiedKFold

# ==============================================================================
# 2. CONFIGURACI√ìN
# ==============================================================================

FEATURE_NAMES = [
    "rango_r_beta_alfa", "rango_t_e_beta", "rango_r_e_beta", "resistencia_inicial",
    "k4", "k3", "rango_intercuartilico", "desv_pre_mitad_t",
    "resistencia_ultima", "desv", "pendiente_V", "rms",
    "rango_rmax_rmin", "r_mean_post_max", "r_mean", "desv_R_pre_max",
    "pendientes_negativas_post", "rango_tiempo_max_min", "area_bajo_curva", "area_pre_mitad",
    "area_post_mitad", "max_curvatura", "num_puntos_inflexion", "max_jerk",
    "mediana", "varianza", "asimetria", "curtosis",
    "num_picos", "num_valles", "q", "m_min_cuadrados"
]

# Configuraci√≥n basada en el reporte
TEST_SIZE_RATIO = 0.30  # Reservamos un poco m√°s para test dado N peque√±o, pero balanceado
RANDOM_STATE_SEED = 42
N_SPLITS_CV = 5
N_REPEATS_CV = 5        # Robustez extra para Small Data
FBETA_BETA = 2          # Prioridad Recall
PRECISION_MINIMA = 0.70 # Restricci√≥n operativa estricta

# ==============================================================================
# 3. FUNCIONES DE PREPROCESAMIENTO (INVARIANTES)
# ==============================================================================
# Nota: Se mantienen las funciones de extracci√≥n de caracter√≠sticas originales.
# Solo se incluye el c√≥digo necesario para que el script sea ejecutable.

def leer_archivo():
    print("Selecciona el archivo CSV que contiene los datos...")
    root = tk.Tk()
    root.withdraw()
    ruta_csv = filedialog.askopenfilename(filetypes=[("Archivos CSV", "*.csv")])
    if not ruta_csv: return None
    try:
        # Ajustar skiprows seg√∫n tu CSV real
        df = pd.read_csv(ruta_csv, encoding="utf-8", sep=";", on_bad_lines="skip", 
                         header=None, decimal=",", skiprows=3)
        return df
    except Exception as e:
        print(f"Error: {e}")
        return None

def calcular_pendiente(resistencias, tiempos):
    """Calcula la pendiente (tasa de cambio) entre valores consecutivos."""
    if len(resistencias) <= 1 or len(tiempos) <= 1:
        return [0]
    pendientes = []
    for i in range(len(resistencias) - 1):
        delta_t = tiempos[i + 1] - tiempos[i]
        delta_r = resistencias[i + 1] - resistencias[i]
        if delta_t == 0:
            pendiente_actual = 0
        else:
            pendiente_actual = (delta_r / delta_t) * 100
        pendientes.append(round(np.nan_to_num(pendiente_actual, nan=0), 2))
    return pendientes

def calcular_derivadas(resistencias, tiempos):
    """Calcula la 1ra, 2da y 3ra derivada de la curva R(t)."""
    if len(resistencias) <= 1 or len(tiempos) <= 1:
        return np.array([0]), np.array([0]), np.array([0])
    primera_derivada = np.gradient(resistencias, tiempos)
    segunda_derivada = np.gradient(primera_derivada, tiempos)
    tercera_derivada = np.gradient(segunda_derivada, tiempos)
    return (
        np.nan_to_num(primera_derivada, nan=0),
        np.nan_to_num(segunda_derivada, nan=0),
        np.nan_to_num(tercera_derivada, nan=0)
    )

def preprocesar_dataframe_inicial(df):
    """Limpia el DataFrame crudo."""
    new_df = df.iloc[:, [0, 8, 9, 10, 20, 27, 67, 98]]
    new_df = new_df.iloc[:-2]
    new_df.columns = ["id punto", "Ns", "Corrientes inst.", "Voltajes inst.", "KAI2", "Ts2", "Fuerza", "Etiqueta datos"]
    for col in ["KAI2", "Ts2", "Fuerza"]:
        new_df[col] = pd.to_numeric(new_df[col], errors='coerce')
    float_cols = new_df.select_dtypes(include='float64').columns
    new_df = new_df.round({col: 4 for col in float_cols})
    new_df.index = range(1, len(new_df) + 1)
    for col in df.columns:
        if df[col].dtype == object:
            try:
                df[col] = df[col].str.replace(',', '.', regex=False).astype(float)
            except:
                pass
    return new_df

def extraer_features_fila_por_fila(new_df):
    """Itera sobre cada fila y calcula el vector de 32 caracter√≠sticas."""
    X_calculado = []
    y_calculado = []
    print(f"Iniciando c√°lculo de features para {len(new_df)} puntos de soldadura...")

    for i in new_df.index:
        # --- 1. Leer series temporales ---
        datos_voltaje = new_df.loc[i, "Voltajes inst."]
        datos_corriente = new_df.loc[i, "Corrientes inst."]
        if pd.isna(datos_voltaje) or pd.isna(datos_corriente):
            print(f"Advertencia: Datos nulos en fila {i}. Saltando.")
            continue
            
        valores_voltaje = [float(v) for v in datos_voltaje.split(';') if v.strip()]
        valores_voltaje = [round(v, 0) for v in valores_voltaje]
        valores_corriente = [0.001 if float(v) == 0 else float(v) for v in datos_corriente.split(';') if v.strip()]
        valores_corriente = [round(v, 0) for v in valores_corriente]

        # --- 2. Calcular Resistencia y Tiempo ---
        valores_resistencia = [v / c if c != 0 else 0 for v, c in zip(valores_voltaje, valores_corriente)]
        valores_resistencia = [round(r, 2) for r in valores_resistencia]
        valores_resistencia.append(0)
        ns = int(new_df.loc[i, "Ns"])
        ts2 = int(new_df.loc[i, "Ts2"])
        t_soldadura = (np.linspace(0, ts2, ns + 1)).tolist()

        if len(t_soldadura) != len(valores_resistencia):
            min_len = min(len(t_soldadura), len(valores_resistencia))
            t_soldadura = t_soldadura[:min_len]
            valores_resistencia = valores_resistencia[:min_len]
            valores_voltaje = valores_voltaje[:min_len]
        if not t_soldadura:
            print(f"Advertencia: Fila {i} sin datos de series temporales. Saltando.")
            continue

        # --- 3. Puntos clave R(t) ---
        resistencia_max = max(valores_resistencia)
        I_R_max = np.argmax(valores_resistencia)
        t_R_max = int(t_soldadura[I_R_max])
        r0 = valores_resistencia[0]
        t0 = t_soldadura[0]
        r_e = valores_resistencia[-2]
        t_e = t_soldadura[-2]
        resistencia_min = min(valores_resistencia[:-1])
        t_min = np.argmin(valores_resistencia[:-1])
        t_soldadura_min = t_soldadura[t_min]
        
        # --- 4. Par√°metros escalares ---
        kAI2 = new_df.loc[i, "KAI2"]
        f = new_df.loc[i, "Fuerza"]
        
        # --- 5. C√°lculo de Features (L√≥gica original) ---
        q = np.nan_to_num(((((kAI2 * 1000.0) ** 2) * (ts2 / 1000.0)) / (f * 10.0)), nan=0)
        area_bajo_curva = np.nan_to_num(np.trapz(valores_resistencia, t_soldadura), nan=0)
        resistencia_ultima = valores_resistencia[-2]
        try:
            delta_t_k4 = t_e - t_R_max
            k4 = 0 if delta_t_k4 == 0 else ((r_e - resistencia_max) / delta_t_k4) * 100
        except ZeroDivisionError: k4 = 0
        k4 = np.nan_to_num(k4, nan=0)
        try:
            delta_t_k3 = t_R_max - t0
            k3 = 0 if delta_t_k3 == 0 else ((resistencia_max - r0) / delta_t_k3) * 100
        except ZeroDivisionError: k3 = 0
        k3 = np.nan_to_num(k3, nan=0)
        desv = np.nan_to_num(np.std(valores_resistencia), nan=0)
        rms = np.nan_to_num(np.sqrt(np.mean(np.square(valores_resistencia))), nan=0)
        rango_tiempo_max_min = np.nan_to_num(t_R_max - t_soldadura_min, nan=0)
        rango_rmax_rmin = np.nan_to_num(resistencia_max - resistencia_min, nan=0)
        voltaje_max = max(valores_voltaje)
        t_max_v = np.argmax(valores_voltaje)
        t_voltaje_max = t_soldadura[t_max_v]
        voltaje_final = valores_voltaje[-2]
        t_voltaje_final = t_soldadura[-2]
        try:
            delta_t_v = t_voltaje_max - t_voltaje_final
            pendiente_V = 0 if delta_t_v == 0 else ((voltaje_max - voltaje_final) / delta_t_v)
        except ZeroDivisionError: pendiente_V = 0
        pendiente_V = np.nan_to_num(pendiente_V, nan=0)
        r_mean_post_max = np.nan_to_num(np.mean(valores_resistencia[I_R_max:]), nan=0)
        resistencia_inicial = np.nan_to_num(r0, nan=2000)
        r_mean = np.nan_to_num(np.mean(valores_resistencia[:-1]), nan=0)
        rango_r_beta_alfa = np.nan_to_num(resistencia_max - r0, nan=0)
        rango_r_e_beta = np.nan_to_num(r_e - resistencia_max, nan=0)
        rango_t_e_beta = np.nan_to_num(t_e - t_R_max, nan=0)
        desv_R = np.nan_to_num(np.std(valores_resistencia[:I_R_max]), nan=0)
        pendientes = calcular_pendiente(valores_resistencia, t_soldadura)
        pendientes_post_max = pendientes[I_R_max:]
        pendientes_negativas_post = sum(1 for p in pendientes_post_max if p < 0)
        valores_resistencia_hasta_R_max = valores_resistencia[:I_R_max + 1]
        valores_tiempo_hasta_R_max = t_soldadura[:I_R_max + 1]
        area_pre_mitad = np.nan_to_num(np.trapz(valores_resistencia_hasta_R_max, valores_tiempo_hasta_R_max), nan=0)
        valores_resistencia_desde_R_max = valores_resistencia[I_R_max:]
        valores_tiempo_desde_R_max = t_soldadura[I_R_max:]
        area_post_mitad = np.nan_to_num(np.trapz(valores_resistencia_desde_R_max, valores_tiempo_desde_R_max), nan=0)
        try:
            desv_pre_mitad_t = np.nan_to_num(np.std(valores_resistencia_hasta_R_max), nan=0)
        except ValueError: desv_pre_mitad_t = 0
        primera_derivada, segunda_derivada, tercera_derivada = calcular_derivadas(valores_resistencia, t_soldadura)
        try:
            max_curvatura = np.nan_to_num(np.max(np.abs(segunda_derivada)), nan=0)
            puntos_inflexion = np.where(np.diff(np.sign(segunda_derivada)))[0]
            num_puntos_inflexion = np.nan_to_num(len(puntos_inflexion), nan=0)
            max_jerk = np.nan_to_num(np.max(np.abs(tercera_derivada)), nan=0)
        except ValueError: max_curvatura, num_puntos_inflexion, max_jerk = 0, 0, 0
        try:
            mediana = np.nan_to_num(np.median(valores_resistencia), nan=0)
            varianza = np.nan_to_num(np.var(valores_resistencia), nan=0)
            rango_intercuartilico = np.nan_to_num((np.percentile(valores_resistencia, 75) - np.percentile(valores_resistencia, 25)), nan=0)
            asimetria = np.nan_to_num(skew(valores_resistencia), nan=0)
            curtosis = np.nan_to_num(kurtosis(valores_resistencia), nan=0)
        except (ValueError, IndexError): mediana, varianza, rango_intercuartilico, asimetria, curtosis = 0, 0, 0, 0, 0
        valores_resistencia_np = np.array(valores_resistencia)
        picos, _ = find_peaks(valores_resistencia_np, height=0)
        valles, _ = find_peaks(-valores_resistencia_np)
        num_picos = np.nan_to_num(len(picos), nan=0)
        num_valles = np.nan_to_num(len(valles), nan=0)
        t_mean = np.nan_to_num(np.mean(t_soldadura), nan=0)
        r_mean_ols = np.nan_to_num(np.mean(valores_resistencia), nan=0)
        numerador = sum((r_mean_ols - ri) * (t_mean - ti) for ri, ti in zip(valores_resistencia, t_soldadura))
        denominador = sum((r_mean_ols - ri) ** 2 for ri in valores_resistencia)
        m_min_cuadrados = 0 if denominador == 0 else (numerador / denominador)

        # --- 6. Ensamblar vector de caracter√≠sticas y etiqueta ---
        X_calculado.append([
            float(rango_r_beta_alfa), float(rango_t_e_beta), float(rango_r_e_beta),
            float(resistencia_inicial), float(k4), float(k3),
            float(rango_intercuartilico), float(desv_pre_mitad_t), float(resistencia_ultima),
            float(desv), float(pendiente_V), float(rms),
            float(rango_rmax_rmin), float(r_mean_post_max), float(r_mean),
            float(desv_R), float(pendientes_negativas_post), float(rango_tiempo_max_min),
            float(area_bajo_curva), float(area_pre_mitad), float(area_post_mitad),
            float(max_curvatura), float(num_puntos_inflexion), float(max_jerk),
            float(mediana), float(varianza), float(asimetria),
            float(curtosis), float(num_picos), float(num_valles),
            float(q), float(m_min_cuadrados)
        ])
        y_calculado.append(int(new_df.loc[i, "Etiqueta datos"]))
        
    print("C√°lculo de features completado.")
    return np.array(X_calculado), np.array(y_calculado)

# ==============================================================================
# 4. PIPELINE DE APRENDIZAJE BAYESIANO
# ==============================================================================

def paso_1_cargar_y_preparar_datos(feature_names):
    df_raw = leer_archivo()
    if df_raw is None: return None, None
    
    # Asumimos que tienes las funciones de extracci√≥n definidas arriba o importadas
    # Aqu√≠ simplifico llamando a las funciones que definiste en tu prompt original
    # IMPORTANTE: Aseg√∫rate de que preprocesar_dataframe_inicial y extraer... est√©n disponibles
    try:
        df_pre = preprocesar_dataframe_inicial(df_raw)
        X_arr, y_arr = extraer_features_fila_por_fila(df_pre)
    except NameError:
        print("Error: Funciones de extracci√≥n no definidas en este contexto.")
        return None, None

    if X_arr.size == 0: return None, None
    
    X = pd.DataFrame(X_arr, columns=feature_names)
    y = pd.Series(y_arr, name="Etiqueta_Defecto")
    
    print(f"Datos cargados: {len(X)} muestras. Tasa defecto: {y.mean():.2%}")
    return X, y

def paso_2_dividir_datos(X, y):
    """Divisi√≥n estratificada simple. El escalado se mueve al Pipeline."""
    return train_test_split(
        X, y, test_size=TEST_SIZE_RATIO, 
        random_state=RANDOM_STATE_SEED, stratify=y
    )

def paso_3_entrenar_gpc_ard(X_train, y_train):
    """
    MODIFICACI√ìN: Implementaci√≥n de GPC con Kernel ARD.
    Se elimina SMOTE y RandomForest. Se introduce StandardScaler.
    """
    print("\n--- Iniciando Entrenamiento Bayesiano (GPC + ARD) ---")
    
    print("\n" + "="*60)
    print("--- PASO 3: ENTRENAMIENTO PROCESO GAUSSIANO (GPC + ARD) ---")
    print("="*60)
    
    # 1. Definici√≥n del Kernel Compuesto
    # K = (Constante * Matern) + WhiteNoise
    # length_scale vector de 32 unos inicializa ARD (una escala por feature)
    dims = X_train.shape[1]
    
    # Matern nu=1.5 es ideal para funciones rugosas (f√≠sica real con ruido)
    # Bounds restringidos (1e-2 a 1e2) para evitar inestabilidad num√©rica en small data
    kernel_matern = 1.0 * Matern(
        length_scale=np.ones(dims), 
        length_scale_bounds=(1e-2, 1e2), 
        nu=1.5 
    )
    
    # WhiteKernel maneja el ruido aleatorio del sensor (regularizaci√≥n)
    kernel_noise = WhiteKernel(noise_level=1e-2, noise_level_bounds=(1e-5, 1e-1))
    
    kernel_total = kernel_matern + kernel_noise

    # 2. Definici√≥n del Pipeline
    # StandardScaler es CR√çTICO para kernels isotr√≥picos/anisotr√≥picos
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('gpc', GaussianProcessClassifier(
            kernel=kernel_total,
            n_restarts_optimizer=10, # Alto n√∫mero de reinicios para evitar m√≠nimos locales
            random_state=RANDOM_STATE_SEED,
            n_jobs=-1,
        ))
    ])

    # 3. Validaci√≥n Cruzada y GridSearch (M√≠nimo)
    # En GPC, los par√°metros se aprenden maximizando la Verosimilitud Marginal.
    # El GridSearch aqu√≠ es solo para validar estabilidad, no es estrictamente necesario
    # para "tunear" como en RF, pero √∫til para comparar configuraciones si se desea.
    
    rskf = RepeatedStratifiedKFold(n_splits=N_SPLITS_CV, n_repeats=N_REPEATS_CV, random_state=RANDOM_STATE_SEED)
    f2_scorer = make_scorer(fbeta_score, beta=FBETA_BETA)

    # Opcional: Podr√≠amos probar nu=2.5 vs 1.5, pero 1.5 es robusto seg√∫n reporte.
    # Dejamos el grid vac√≠o para solo entrenar y evaluar con CV robusta.
    param_grid = {} 

    print("Optimizando Verosimilitud Marginal Logar√≠tmica (LML)...")
    search = GridSearchCV(
        pipeline, param_grid, cv=rskf, scoring=f2_scorer, n_jobs=-1, verbose=3
    )
    
# --- CRON√ìMETRO: INICIO ---
    start_time = time.time()
    
    # Ejecutar entrenamiento
    search.fit(X_train, y_train)
    
    # --- CRON√ìMETRO: FIN (Aqu√≠ se define la variable que faltaba) ---
    elapsed_time = time.time() - start_time
    
    print("\n" + "-"*60)
    print(f"¬°ENTRENAMIENTO COMPLETADO!")
    # Ahora s√≠ funcionar√° porque elapsed_time ya existe
    print(f"Tiempo total de ejecuci√≥n: {elapsed_time:.2f} segundos")
    print("-" * 60)
    
    print(f"Mejor F2 (CV Promedio): {search.best_score_:.4f}")
    print("Kernel Optimizado:", search.best_estimator_.named_steps['gpc'].kernel_)
    
    return search.best_estimator_

def paso_4_analisis_relevancia_ard(modelo, feature_names):
    """
    MODIFICACI√ìN: Interpretaci√≥n de ARD.
    En GPC, Feature Importance ~ 1 / (Length Scale).
    Escala grande = Feature irrelevante (la funci√≥n no cambia al movernos en esa dimensi√≥n).
    Escala peque√±a = Feature cr√≠tica.
    """
    print("\n--- An√°lisis de Relevancia Autom√°tica (ARD) ---")
    
    # Acceder al kernel entrenado dentro del pipeline
    gpc_model = modelo.named_steps['gpc']
    
    # La estructura del kernel suele ser: Sum(Product(Constant, Matern), WhiteKernel)
    # Necesitamos navegar hasta el componente Matern para sacar las escalas.
    # gpc_model.kernel_ es el kernel final optimizado.
    
    kernel_opt = gpc_model.kernel_
    
    # Navegaci√≥n robusta para encontrar length_scale
    length_scales = None
    
    # Intentamos acceder a k1 (que suele ser Product: Constant * Matern)
    if hasattr(kernel_opt, 'k1'):
        sub_k = kernel_opt.k1
        if hasattr(sub_k, 'k2'): # k2 suele ser Matern
             length_scales = sub_k.k2.length_scale
    
    if length_scales is None:
        print("No se pudo extraer autom√°ticamente la estructura del kernel para ARD.")
        # Intento directo si la estructura es simple
        try:
             length_scales = kernel_opt.k1.k2.length_scale
        except:
             print("Estructura de kernel compleja. Saltando gr√°fico ARD.")
             return

    # Calculamos la "Relevancia" como la inversa de la escala
    # Relevancia = 1 / length_scale
    relevancia = 1.0 / length_scales
    
    # Normalizar para visualizaci√≥n (0 a 100)
    relevancia_norm = 100 * (relevancia / np.max(relevancia))

    df_ard = pd.DataFrame({
        'Feature': feature_names,
        'Relevancia_ARD': relevancia_norm,
        'Length_Scale': length_scales
    }).sort_values(by='Relevancia_ARD', ascending=False)

    print(df_ard.head(10))

    plt.figure(figsize=(10, 8))
    sns.barplot(data=df_ard.head(15), x='Relevancia_ARD', y='Feature', palette='viridis')
    plt.title('Top 15 Caracter√≠sticas m√°s Relevantes (ARD - Proceso Gaussiano)')
    plt.xlabel('Relevancia Inversa Normalizada (1/Length_Scale)')
    plt.tight_layout()
    plt.show()

def paso_5_optimizar_umbral_probabilistico(modelo, X_train, y_train):
    """
    MODIFICACI√ìN: Optimizaci√≥n basada en probabilidades puras de GPC.
    Objetivo: Max(Recall) sujeto a Precision >= 0.7.
    """
    print(f"\n--- Calibraci√≥n de Umbral (Restricci√≥n: Precisi√≥n >= {PRECISION_MINIMA}) ---")
    
    # CAMBIO CR√çTICO AQU√ç:
    # Usamos StratifiedKFold est√°ndar (sin repeticiones) para que cross_val_predict funcione.
    # Esto garantiza que cada muestra tenga exactamente UNA predicci√≥n.
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE_SEED)
    
    y_probas = cross_val_predict(
        modelo, X_train, y_train, cv=skf, method='predict_proba', n_jobs=-1
    )[:, 1]

    umbrales = np.linspace(0.1, 0.95, 1000)
    best_thresh = 0.5
    best_recall = 0.0
    final_precision = 0.0
    
    precision_list, recall_list = [], []

    found_feasible = False
    
    for t in umbrales:
        preds = (y_probas >= t).astype(int)
        p = precision_score(y_train, preds, zero_division=0)
        r = recall_score(y_train, preds, zero_division=0)
        
        precision_list.append(p)
        recall_list.append(r)
        
        if p >= PRECISION_MINIMA:
            found_feasible = True
            if r > best_recall:
                best_recall = r
                best_thresh = t
                final_precision = p
            # Si el recall es igual, preferimos umbral menor (m√°s robusto)
            elif r == best_recall and r > 0:
                 pass 

    if not found_feasible:
        print("ADVERTENCIA: Ning√∫n umbral cumple la restricci√≥n de precisi√≥n. Usando 0.5.")
        best_thresh = 0.5
    else:
        print(f"Umbral √ìptimo Encontrado: {best_thresh:.4f}")
        print(f"M√©tricas Esperadas -> Recall: {best_recall:.4f} | Precision: {final_precision:.4f}")

    # Gr√°fico de Trade-off
    plt.figure(figsize=(10, 6))
    plt.plot(umbrales, precision_list, label='Precision', color='blue')
    plt.plot(umbrales, recall_list, label='Recall', color='green')
    plt.axvline(best_thresh, color='red', linestyle='--', label=f'Optimum ({best_thresh:.2f})')
    plt.axhline(PRECISION_MINIMA, color='gray', linestyle=':', label='Min Precision')
    plt.xlabel('Probabilidad de Corte')
    plt.title('Curvas de Operaci√≥n para Calibraci√≥n de Umbral')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    return best_thresh

def paso_6_evaluacion_final(modelo, X_test, y_test, umbral, feature_names):
    """
    Evaluaci√≥n final utilizando el umbral calibrado.
    """
    print("\n--- Validaci√≥n Final en Test Set ---")
    
    # Probabilidades del GPC (naturalmente bien calibradas)
    probs_test = modelo.predict_proba(X_test)[:, 1]
    preds_test = (probs_test >= umbral).astype(int)
    
    print(classification_report(y_test, preds_test, target_names=["OK", "DEFECTO"]))
    
    cm = confusion_matrix(y_test, preds_test)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=["Pred OK", "Pred Defecto"], 
                yticklabels=["Real OK", "Real Defecto"])
    plt.title(f'Matriz de Confusi√≥n (Umbral {umbral:.3f})')
    plt.show()
    
    # Guardado
    artefactos = {
        "modelo_pipeline": modelo,
        "umbral_optimo": umbral,
        "feature_names": feature_names
    }
    with open('modelo_GPC_ARD_soldadura.pkl', 'wb') as f:
        pickle.dump(artefactos, f)
    print("Modelo GPC guardado exitosamente.")

# ==============================================================================
# MAIN
# ==============================================================================

def main():
    # 1. Prepara datos
    try:
        X, y = paso_1_cargar_y_preparar_datos(FEATURE_NAMES)
    except NameError:
        print("Por favor, define las funciones de extracci√≥n de caracter√≠sticas (copiar del script anterior).")
        return

    if X is None: return

    # 2. Dividir
    X_train, X_test, y_train, y_test = paso_2_dividir_datos(X, y)
    
    # ---------------------------------------------------------
    # 2. RONDA 1: ENTRENAMIENTO DE DESCUBRIMIENTO (32 FEATURES)
    # ---------------------------------------------------------
    print("\nüî¥ RONDA 1: Entrenando con TODAS las variables para descubrir relevancia...")
    modelo_exploratorio = paso_3_entrenar_gpc_ard(X_train, y_train)
    
    # ---------------------------------------------------------
    # 3. SELECCI√ìN DE VARIABLES (ARD)
    # ---------------------------------------------------------
    print("\nüîç ANALIZANDO QU√â VARIABLES IMPORTAN...")
    
    # Extraemos las escalas del modelo exploratorio
    # (Aseg√∫rate de que esta l√≥gica coincida con tu estructura de kernel)
    try:
        scales = modelo_exploratorio.named_steps['gpc'].kernel_.k1.k2.length_scale
    except:
        scales = modelo_exploratorio.named_steps['gpc'].kernel_.length_scale
        
    relevancia = 1.0 / scales
    relevancia_norm = 100 * (relevancia / np.max(relevancia))
    
    # CRITERIO: Nos quedamos con las que tengan > 10% de importancia relativa
    indices_vip = np.where(relevancia_norm >= 10.0)[0]
    features_vip = np.array(FEATURE_NAMES)[indices_vip]
    
    print(f"‚úÖ SELECCIONADAS {len(features_vip)} VARIABLES CR√çTICAS:")
    print(list(features_vip))
    
    # ---------------------------------------------------------
    # 4. PREPARAR DATOS VIP
    # ---------------------------------------------------------
    # Filtramos los datasets para quedarnos solo con las columnas VIP
    # IMPORTANTE: .iloc para filtrar por posici√≥n de columna
    X_train_vip = X_train.iloc[:, indices_vip]
    X_test_vip  = X_test.iloc[:, indices_vip]

    # ---------------------------------------------------------
    # 5. RONDA 2: ENTRENAMIENTO FINAL (SOLO FEATURES VIP)
    # ---------------------------------------------------------
    print("\nüü¢ RONDA 2: Re-entrenando modelo FINAL (Ultrarrobusto)...")
    
    # ¬°REUTILIZAMOS LA MISMA FUNCI√ìN DE ENTRENAMIENTO!
    # Como X_train_vip tiene menos columnas, la funci√≥n se adapta sola.
    modelo_final = paso_3_entrenar_gpc_ard(X_train_vip, y_train)

    # ---------------------------------------------------------
    # 6. OPTIMIZACI√ìN DE UMBRAL Y EVALUACI√ìN (CON MODELO FINAL)
    # ---------------------------------------------------------
    # Usamos el modelo final y los datos reducidos
    umbral_optimo = paso_5_optimizar_umbral_probabilistico(modelo_final, X_train_vip, y_train)
    
    paso_6_evaluacion_final(modelo_final, X_test_vip, y_test, umbral_optimo, features_vip)
    
if __name__ == "__main__":
    main()